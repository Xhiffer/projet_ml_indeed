{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A IMPORTER\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import unidecode\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On charge notre dataset. Puis on créé un autre dataset reprenant les features qui nous intéresse pour appliquer nos processus NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('indeed6.json', encoding = 'utf-8', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_to_class = data[['titre', 'texte']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous avons décidé d'utiliser la libraire spaCy. C'est elle qui nous permettra de parseriser nos descriptions et nos titres, retirer les stop words, et tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "#Chargeons notre vocabulaire spaCy\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chargeons nos stopwords qu'on mettra dans une liste\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "french_stopwords = list(spacy.lang.fr.stop_words.STOP_WORDS)\n",
    "english_stopwords = list(spacy.lang.en.stop_words.STOP_WORDS)\n",
    "stop_words = french_stopwords + english_stopwords\n",
    "\n",
    "#J'importe la ponctuation qui nous servira au traitement du texte\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "\n",
    "#Ainsi que notre Parser qui comprend la structure grammatical de notre texte\n",
    "from spacy.lang.fr import French\n",
    "parser = French()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, on avait opté pour utiliser strictement spaCy pour le traitement global. Toutefois, il restait imparfait avec beaucoup de \"déchet\" dans nos tokens.\n",
    "On choisit donc de subdiviser à nouveau notre traitement. \n",
    "Tout d'abord on retire les caractères indésirables. Puis, dans la mesure où on a déjà créé des nouvelles features grâce au regex, on supprime ces éléments de nos données afin d'éviter tout risque de colinéarité dans l'entraînement de nos modèles de machine learning.\n",
    "Enfin on tokenize nos textes et on nettoie à nouveau les caractères indésirables qui ont pu se glisser dedans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import digits\n",
    "\n",
    "#Ces caractères indésirables ne nous fournissent pas d'information pertinente. On les retire\n",
    "def clean_and_lower(string):\n",
    "    forbid_car= [\":\", \";\", \",\", \"&\", \"(\", \")\",\n",
    "                '\"', \"!\", \"?\", \"*\", \"-\", \"\\n\", \n",
    "                \"...\", \"/\",\"'\"]\n",
    "    for car in forbid_car:\n",
    "            cleaned_string = string.replace(car, ' ')\n",
    "    return cleaned_string\n",
    "\n",
    "#On retire les termes déjà étudié lors de nos regex, et qui sont déjà des features\n",
    "def terme_a_retirer(string):\n",
    "        string = re.sub(pattern = '(jeune[es().]* diplôme[es().]*|junior[s]*|novice[s]*| débutant[es().]*)', repl = '', string = string, flags=re.IGNORECASE)\n",
    "        string = re.sub(pattern = '(expérimenté[es().]*|senior[s]*|confirmé[es().]*)', repl = '', string = string, flags=re.IGNORECASE)\n",
    "        string = re.sub(pattern = '(licence|bac[ +]*[23/]+|iut|dut|bts| bsc|master|bac[ +]*5|ingénieur[.()es]*|grande[s]* école[s]*| msc|doctorat[s]*|docteur[.()es]*|ph[.]*d|thèse[s]*|bac)', repl = '', string = string, flags=re.IGNORECASE)\n",
    "        string = re.sub(pattern = '(minimum [1-9]*[/aou-]*\\d+ an[nées]* d\\'expérience|[1-9]*[/aou-]*\\d+ an[nées]* d\\'expérience|[1-9]*[/aou-]*\\d+ an[nées]* minimum d\\'expérience|expérience minimum de *[1-9]*[/aou-]*\\d+ an[nees]|expérience|d\\'expérience)', repl = '', string = string, flags=re.IGNORECASE)\n",
    "        string = re.sub(pattern = '(nantes|bordeaux|paris|île-de-france|lyon|toulouse)', repl = '', string = string, flags=re.IGNORECASE)\n",
    "        string = re.sub(pattern = '[\\d+ ]*[ \\-k€]*[\\d+ ]*\\d+,?\\d+[ ]*[k€$]+', repl = '', string = string, flags=re.IGNORECASE)\n",
    "        string = re.sub(pattern = '(cdi|CDI|Cdi|cdd|CDD|Cdd|stage[s]*|stagiaire[s]*|intern[s]*|internship[s]*|freelance|Freelance|FREELANCE||indépendant[s]*|par an|/an|temps plein|3dexperience)', repl = '', string = string, flags=re.IGNORECASE)\n",
    "        string = re.sub(pattern = '(h[/ \\-]*f|f[/ \\-]*h)', repl = '', string = string, flags=re.IGNORECASE)\n",
    "        string = re.sub(pattern = '(salaire|[\\d+]+[ ]*an[nées]*|[\\d+]+[ ]*jour[s]|/mois|/jour[s]*|/semaine[s]*|[\\d+]+ mois|[\\d+]+ semaine[s]*|mois|semaine[s]*|jour[s]*])', repl = '', string = string, flags=re.IGNORECASE)\n",
    "        string = re.sub(pattern = '[\\d+]+[]*[èe]*[mes]*', repl = '', string = string, flags=re.IGNORECASE)\n",
    "        string = re.sub(pattern = r'\\d+', repl = '', string = string, flags=re.IGNORECASE)\n",
    "        string = re.sub(pattern = '[\\w\\.-]+@[\\w\\.-]+', repl = '', string = string, flags=re.IGNORECASE)\n",
    "        return string\n",
    "\n",
    "#On tokenize notre texte, on retire les pronoms, stop words, et on réduit chaque terme à sa racine\n",
    "def tokenize(string):\n",
    "    parsed_string = parser(string)\n",
    "    tokenized_string = [word.lemma_.lower().strip() if word.lemma_ != '-PRON-' else word.lower_ for word in parsed_string]\n",
    "    cleaned_tokenized_string = [word for word in tokenized_string if word not in punctuations and word not in stop_words]\n",
    "    return cleaned_tokenized_string\n",
    "\n",
    "#On retire la ponctuaction indésirable qui peut rester dans nos tokens\n",
    "def strip_final_punctuation(list_of_tokens):\n",
    "    forbid_car= [\":\", \";\", \",\", \"&\", \"(\", \")\",\n",
    "                '\"', \"!\", \"?\", \"*\", \"-\", \n",
    "                \"...\", \"/\",\"'\", \"\\\\\", \"·\", \".\", \"∙\", \"•\"\n",
    "                \"–\"]\n",
    "    \n",
    "    for i in range(len(list_of_tokens)):\n",
    "        for car in forbid_car:\n",
    "            list_of_tokens[i] = list_of_tokens[i].replace(car, '')\n",
    "    return list_of_tokens\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit ensuite une fonction finale de nettoyage qui tournera sur nos descriptions et sur nos titres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(string):\n",
    "    cleaned_string = clean_and_lower(string)\n",
    "    cleaned_string = terme_a_retirer(cleaned_string)\n",
    "    tokenized_string = tokenize(cleaned_string)\n",
    "    tokenized_string = strip_final_punctuation(tokenized_string)\n",
    "    return tokenized_string\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des descriptions avec SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Tokenize, clean, lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On tokenize complètement avec spaCy nos descriptions\n",
    "\n",
    "job_to_class['spacy_description'] = [cleaning(job_to_class.loc[i, 'texte']) for i in range(len(job_to_class['texte']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Create the TF-IDF term-documents matrix with the processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nous faut maintenant évaluer le poid relatif de chaque terme dans nos descriptions. On choisit donc de vectoriser en utilisant une matrice Tf-Idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'initialisation des paramètres de notre vectorisation reste encore une question ouverte. L'idéal aurait été de tester automatiquement plusieurs nombre maximal de features et plusieurs n_gram, les stocker dans un log, et choisir le plus adapté. Faute de temps, on a choisi un grand nombre de features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On initialise nos paramètres pour la vectorisation\n",
    "\n",
    "max_features_description = 5000\n",
    "min_n_description = 1\n",
    "max_n_description = 3\n",
    "\n",
    "#On crée une fonction inutile. En effet TfidfVectorizer ne peux pas gérer des documents déjà tokenisés\n",
    "\n",
    "def dummy_fun(doc): \n",
    "    return doc\n",
    "\n",
    "#On vectorise\n",
    "\n",
    "tfidf_vectorizer_description = TfidfVectorizer( analyzer = 'word',\n",
    "                                    tokenizer = dummy_fun,\n",
    "                                    preprocessor = dummy_fun,\n",
    "                                    token_pattern = None,\n",
    "                                    max_df = 1.0, min_df = 1,\n",
    "                                    max_features = max_features_description, sublinear_tf = True,\n",
    "                                    ngram_range=(min_n_description, max_n_description))\n",
    "\n",
    "\n",
    "tfidf_matrix_description = tfidf_vectorizer_description.fit_transform(job_to_class['spacy_description'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Get the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise maintenant notre matrice term-document pour extraire les features les plus importantes de nos descriptions. C'est celles-ci qui vont ensuite nous servir pour l'extraction de topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_description = tfidf_vectorizer_description.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Find the (features x topics) matric with the LDA/LSI/NMF and generate groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois nos features extraites, on choisit d'utiliser trois méthodes pour extraire les topics: Nonnegative Matrix Factorisation, Latent Dirichlet Allocation et Latent Semantic Index.\n",
    "On créé nos trois matrices d'extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_description = LatentDirichletAllocation(n_components= 10, max_iter=10, learning_method='online',verbose=True)\n",
    "data_lda_description = lda_description.fit_transform(tfidf_matrix_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Negative Matrix Factorization Model\n",
    "nmf_description = NMF(n_components = 10)\n",
    "data_nmf_description = nmf_description.fit_transform(tfidf_matrix_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Semantic Indexing Model using Truncated SVD\n",
    "lsi_description = TruncatedSVD(n_components = 10)\n",
    "data_lsi_description = lsi_description.fit_transform(tfidf_matrix_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On imprime nos divers topics pour observer s'il y a de la cohérence et si c'est exploitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for printing keywords for each topic\n",
    "def selected_topics(model, top_n = 10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(tfidf_vectorizer_description.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LDA_description Model:\")\n",
    "selected_topics(lda_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LSI_description Model:\")\n",
    "selected_topics(lsi_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NMF_description Model:\")\n",
    "selected_topics(nmf_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des titres avec SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qui a été obtenu via le traitement des descriptions n'est finalement pas satisfaisant. On essaye ici de concentrer le traitement sur les titres de chaque annonce. L'intuition c'est que l'information pour clusteriser nos annonces est plus concentrée sur les titres. Et donc la possibilité de différenciation sera plus importante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les étapes étant les mêmes que pour la description, on ne répétera pas les commentaires précédents. On se contentera d'analyser les résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Tokenize, clean, lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_to_class['spacy_titre'] = [cleaning(job_to_class.loc[i, 'titre']) for i in range(len(job_to_class['titre']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Create the TF-IDF term-documents matrix with the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on a fait le choix de réduire les max_features à 500, et le n_gram à 1. En effet, les titres sont bien plus court, et bien plus redondants. Mais là aussi il aurait fallu tester automatiquement plusieurs valeurs et sélectionner celles qui donne les meilleures métriques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On initialise nos paramètres pour la vectorisation\n",
    "\n",
    "max_features_titres = 500\n",
    "min_n_titres = 1\n",
    "max_n_titres = 1\n",
    "\n",
    "#On crée une fonction inutile. En effet TfidfVectorizer ne peux pas gérer des documents déjà tokenisés\n",
    "\n",
    "def dummy_fun(doc): \n",
    "    return doc\n",
    "\n",
    "#On vectorise\n",
    "\n",
    "tfidf_vectorizer_titres = TfidfVectorizer( analyzer = 'word',\n",
    "                                    tokenizer = dummy_fun,\n",
    "                                    preprocessor = dummy_fun,\n",
    "                                    token_pattern = None,\n",
    "                                    max_df = 1.0, min_df = 1,\n",
    "                                    max_features = max_features_titres, sublinear_tf = True,\n",
    "                                    ngram_range=(min_n_titres, max_n_titres))\n",
    "\n",
    "\n",
    "tfidf_matrix_titres = tfidf_vectorizer_titres.fit_transform(job_to_class['spacy_titre'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Get the features !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_titres = tfidf_vectorizer_titres.get_feature_names()\n",
    "features_titres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Find the (features x topics) matric with the LDA/LSI/NMF and generate groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_titres = LatentDirichletAllocation(n_components= 20, max_iter=10, learning_method='online',verbose=True)\n",
    "data_lda_titres = lda_titres.fit_transform(tfidf_matrix_titres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_20 = pd.DataFrame(data_lda_titres)\n",
    "df_topic_20\n",
    "df_topic_20.to_csv(\"topic_20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Negative Matrix Factorization Model\n",
    "nmf_titres = NMF(n_components = 20)\n",
    "data_nmf_titres = nmf_titres.fit_transform(tfidf_matrix_titres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Semantic Indexing Model using Truncated SVD\n",
    "lsi_titres = TruncatedSVD(n_components = 15)\n",
    "data_lsi_titres = lsi_titres.fit_transform(tfidf_matrix_titres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for printing keywords for each topic\n",
    "def selected_topics(model, top_n = 15):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(tfidf_vectorizer_titres.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"LDA_titres Model:\")\n",
    "# type(selected_topics(lda_titres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"LSI_titres Model:\")\n",
    "# selected_topics(lsi_titres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NMF_titres Model:\")\n",
    "print(type(selected_topics(nmf_titres)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_20 = pd.DataFrame(data_nmf_titres)\n",
    "df_topic_20.to_csv(\"topic_20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_20.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'application de notre méthode de NLP sur les titres s'avère donner des extractions de topic plus cohérent que celle sur les descriptions. Toutefois l'ajout de notre matrice NMF dans les features de notre dataset nous fait gagner une accuracy relativement marginale. Et malheureusement les topics ne sont pas suffisamment cohérent pour alimenter la data analysis côté client.\n",
    "Peut-être que la libraire Bert pourrait s'avérer plus utile sur ces deux niveaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
